{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQL_Injection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "21NidijD03uZ",
        "outputId": "1a0da369-fafd-46fb-e92e-6cb94e3802d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#!pip install pandas==1.1.2\n",
        "import pandas as pd\n",
        "import os\n",
        "import math\n",
        "import collections\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "sql_regex = re.compile(\"(?P<UNION>UNION\\s+(ALL\\s+)?SELECT)|(?P<PREFIX>([\\'\\\"\\)]|((\\'|\\\"|\\)|\\d+|\\w+)\\s))(\\|\\|\\&\\&|and|or|as|where|IN\\sBOOLEAN\\sMODE)(\\s|\\()(\\(?\\'?-?\\d+\\'?(=|LIKE|<|>|<=|>=)\\'?-?\\d+|\\(?[\\'\\\"\\\\\\\"]\\S+[\\'\\\"\\\\\\\"](\\s+)?(=|LIKE|<|>|<=|>=)(\\s+)?[\\'\\\"\\\\\\\"]))|(?P<USUAL>([\\'\\\"]\\s*)(\\|\\||\\&\\&|and|or)(\\s*[\\'\\\"])(\\s*[\\'\\\"])=)|(?P<DROP>;\\s*DROP\\s+(TABLE|DATABASE)\\s(IF\\s+EXISTS\\s)?\\S+)|(?P<NOTIN>\\snot\\sin\\s?\\((\\d+|(\\'|\\\")\\w+(\\'|\\\"))\\))|(?P<LIMIT>LIMIT\\s+\\d+(\\s+)?,(\\s+)?\\d+)|GROUP_CONCAT\\((?P<GRPCONCAT>.*?)\\)|(?P<ORDERBY>ORDER\\s+BY\\s+\\d+)|CONCAT\\((?P<CONCAT>.*?)\\)|(?P<CASEWHEN>\\(CASE\\s(\\d+\\s|\\(\\d+=\\d+\\)\\s|NULL\\s)?WHEN\\s(\\d+|\\(?\\d+=\\d+\\)?|NULL)\\sTHEN\\s(\\d+|\\(\\d+=\\d+\\)|NULL)\\sELSE)|(?P<DBNAME>(?:(?:m(?:s(?:ysaccessobjects|ysaces|ysobjects|ysqueries|ysrelationships|ysaccessstorage|ysaccessxml|ysmodules|ysmodules2|db)|aster\\.\\.sysdatabases|ysql\\.db)|s(?:ys(?:\\.database_name|aux)|chema(?:\\W*\\(|_name)|qlite(_temp)?_master)|d(?:atabas|b_nam)e\\W*\\(|information_schema|pg_(catalog|toast)|northwind|tempdb)))|(?P<DATABASE>DATABASE\\(\\))|(?P<DTCNAME>table_name|column_name|table_schema|schema_name)|(?P<CAST>CAST\\(.*AS\\s+\\w+\\))|(?P<INQUERY>\\(SELECT[^a-z_0-9])|(?P<CHRBYPASS>((CHA?R\\(\\d+\\)(,|\\|\\||\\+)\\s?)+)|CHA?R\\((\\d+,\\s?)+\\))|(?P<FROMDB>\\sfrom\\s(dual|sysmaster|sysibm)[\\s.:])|(?P<MYSQLFUNC>[^.](ABS|ACOS|ADDDATE|ADDTIME|AES_DECRYPT|AES_ENCRYPT|ANY_VALUE|ASCII|ASIN|ASYMMETRIC_DECRYPT|ASYMMETRIC_DERIVE|ASYMMETRIC_ENCRYPT|ASYMMETRIC_SIGN|ASYMMETRIC_VERIFY|ATAN|ATAN2|AVG|BENCHMARK|BIN|BIT_AND|BIT_COUNT|BIT_LENGTH|BIT_OR|BIT_XOR|CAST|CEIL|CEILING|CHAR|CHAR_LENGTH|CHARACTER_LENGTH|CHARSET|COALESCE|COERCIBILITY|COLLATION|COMPRESS|CONCAT|CONCAT_WS|CONNECTION_ID|CONV|CONVERT|CONVERT_TZ|COS|COT|COUNT|COUNT|CRC32|CREATE_ASYMMETRIC_PRIV_KEY|CREATE_ASYMMETRIC_PUB_KEY|CREATE_DH_PARAMETERS|CREATE_DIGEST|CURDATE|CURRENT_DATE|CURRENT_TIME|CURRENT_TIMESTAMP|CURRENT_USER|CURTIME|DATABASE|DATE|DATE_ADD|DATE_FORMAT|DATE_SUB|DATEDIFF|DAY|DAYNAME|DAYOFMONTH|DAYOFWEEK|DAYOFYEAR|DECODE|DEFAULT|DEGREES|ELT|ENCODE|EXP|EXPORT_SET|EXTRACT|EXTRACTVALUE|FIELD|FIND_IN_SET|FLOOR|FORMAT|FOUND_ROWS|FROM_BASE64|FROM_DAYS|FROM_UNIXTIME|GeometryCollection|GET_FORMAT|GET_LOCK|GREATEST|GROUP_CONCAT|GTID_SUBSET|GTID_SUBTRACT|HEX|HOUR|IF|IFNULL|IIF|IN|INET_ATON|INET_NTOA|INET6_ATON|INET6_NTOA|INSERT|INSTR|INTERVAL|IS_FREE_LOCK|IS_IPV4|IS_IPV4_COMPAT|IS_IPV4_MAPPED|IS_IPV6|IS_USED_LOCK|ISNULL|JSON_APPEND|JSON_ARRAY|JSON_ARRAY_APPEND|JSON_ARRAY_INSERT|JSON_CONTAINS|JSON_CONTAINS_PATH|JSON_DEPTH|JSON_EXTRACT|JSON_INSERT|JSON_KEYS|JSON_LENGTH|JSON_MERGE|JSON_OBJECT|JSON_QUOTE|JSON_REMOVE|JSON_REPLACE|JSON_SEARCH|JSON_SET|JSON_TYPE|JSON_UNQUOTE|JSON_VALID|LAST_INSERT_ID|LCASE|LEAST|LEFT|LENGTH|LineString|LN|LOAD_FILE|LOCALTIME|LOCALTIMESTAMP|LOCATE|LOG|LOG10|LOG2|LOWER|LPAD|LTRIM|MAKE_SET|MAKEDATE|MAKETIME|MASTER_POS_WAIT|MAX|MBRContains|MBRCoveredBy|MBRCovers|MBRDisjoint|MBREquals|MBRIntersects|MBROverlaps|MBRTouches|MBRWithin|MICROSECOND|MID|MIN|MINUTE|MOD|MONTH|MONTHNAME|MultiLineString|MultiPoint|MultiPolygon|NAME_CONST|NOT IN|NOW|NULLIF|OCT|OCTET_LENGTH|OLD_PASSWORD|ORD|PERIOD_ADD|PERIOD_DIFF|PI|Point|Polygon|POSITION|POW|POWER|PROCEDURE ANALYSE|QUARTER|QUOTE|RADIANS|RAND|RANDOM_BYTES|RELEASE_ALL_LOCKS|RELEASE_LOCK|REPEAT|REPLACE|REVERSE|RIGHT|ROUND|ROW_COUNT|RPAD|RTRIM|SCHEMA|SEC_TO_TIME|SECOND|SESSION_USER|SHA1|SHA2|SIGN|SIN|SLEEP|SOUNDEX|SPACE|SQRT|ST_Area|ST_AsBinary|ST_AsGeoJSON|ST_AsText|ST_Buffer|ST_Buffer_Strategy|ST_Centroid|ST_Contains|ST_ConvexHull|ST_Crosses|ST_Difference|ST_Dimension|ST_Disjoint|ST_Distance|ST_Distance_Sphere|ST_EndPoint|ST_Envelope|ST_Equals|ST_ExteriorRing|ST_GeoHash|ST_GeomCollFromText|ST_GeomCollFromWKB|ST_GeometryN|ST_GeometryType|ST_GeomFromGeoJSON|ST_GeomFromText|ST_GeomFromWKB|ST_InteriorRingN|ST_Intersection|ST_Intersects|ST_IsClosed|ST_IsEmpty|ST_IsSimple|ST_IsValid|ST_LatFromGeoHash|ST_Length|ST_LineFromText|ST_LineFromWKB|ST_LongFromGeoHash|ST_MakeEnvelope|ST_MLineFromText|ST_MLineFromWKB|ST_MPointFromText|ST_MPointFromWKB|ST_MPolyFromText|ST_MPolyFromWKB|ST_NumGeometries|ST_NumInteriorRing|ST_NumPoints|ST_Overlaps|ST_PointFromGeoHash|ST_PointFromText|ST_PointFromWKB|ST_PointN|ST_PolyFromText|ST_PolyFromWKB|ST_Simplify|ST_SRID|ST_StartPoint|ST_SymDifference|ST_Touches|ST_Union|ST_Validate|ST_Within|ST_X|ST_Y|StartPoint|STD|STDDEV|STDDEV_POP|STDDEV_SAMP|STR_TO_DATE|STRCMP|SUBDATE|SUBSTR|SUBSTRING|SUBSTRING_INDEX|SUBTIME|SUM|SYSDATE|SYSTEM_USER|TAN|TIME|TIME_FORMAT|TIME_TO_SEC|TIMEDIFF|TIMESTAMP|TIMESTAMPADD|TIMESTAMPDIFF|TO_BASE64|TO_DAYS|TO_SECONDS|TRIM|TRUNCATE|UCASE|UNCOMPRESS|UNCOMPRESSED_LENGTH|UNHEX|UNIX_TIMESTAMP|UpdateXML|UPPER|USER|UTC_DATE|UTC_TIME|UTC_TIMESTAMP|UUID|UUID_SHORT|VALIDATE_PASSWORD_STRENGTH|VALUES|VAR_POP|VAR_SAMP|VARIANCE|VERSION|WAIT_FOR_EXECUTED_GTID_SET|WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS|WEEK|WEEKDAY|WEEKOFYEAR|WEIGHT_STRING|YEAR|YEARWEEK)\\()|(?P<BOOLEAN>\\'?-?\\d+\\'?(=|LIKE)\\'?-?\\d+($|\\s|\\)|,|--|#)|[\\'\\\"\\\\\\\"]\\S+[\\'\\\"\\\\\\\"](\\s+)?(=|LIKE)(\\s+)?[\\'\\\"\\\\\\\"]\\S+)|(?P<PLAIN>(@|##|#)[A-Z]\\w+|[A-Z]\\w*(?=\\s*\\.)|(?<=\\.)[A-Z]\\w*|[A-Z]\\w*(?=\\()|`(``|[^`])*`|´(´´|[^´])*´|[_A-Z][_$#\\w]*|[가-힣]+)\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "from itertools import groupby\n",
        "\n",
        "def Sql_tokenizer(raw_sql):\n",
        "    if sql_regex.search(raw_sql):\n",
        "        return [tok[0] for tok in groupby([match.lastgroup for match in sql_regex.finditer(raw_sql)])]\n",
        "    else:\n",
        "        return ['PLAIN']\n",
        "\n",
        "def GetTokenSeq(token_list, N):\n",
        "    token_seq = []\n",
        "    for n in range(0,N):\n",
        "        token_seq += zip(*(token_list[i:] for i in range(n+1)))\n",
        "    return [str(tuple) for tuple in token_seq]\n",
        "\n",
        "\n",
        "def G_test_score(count, expected):\n",
        "        if (count == 0):\n",
        "            return 0\n",
        "        else:\n",
        "            return 2.0 * count * math.log(count/expected)\n",
        "\n",
        "\n",
        "def G_test(tokens, types):\n",
        "    tokens_cnt = tokens.value_counts().astype(float)\n",
        "    types_cnt = types.value_counts().astype(float)\n",
        "    total_cnt = float(sum(tokens_cnt))\n",
        "\n",
        "  \n",
        "    token_cnt_table = collections.defaultdict(lambda : collections.Counter())\n",
        "    for _tokens, _types in zip(tokens.values, types.values):\n",
        "        token_cnt_table[_tokens][_types] += 1\n",
        "\n",
        "    \n",
        "    tc_dataframe = pd.DataFrame(token_cnt_table.values(), index=token_cnt_table.keys())\n",
        "    tc_dataframe.fillna(0, inplace=True)\n",
        "\n",
        "   \n",
        "    for column in tc_dataframe.columns.tolist():\n",
        "        #tc_dataframe[column] += 1\n",
        "        tc_dataframe[column+'_exp'] = (tokens_cnt / total_cnt) * types_cnt[column]\n",
        "        tc_dataframe[column+'_GTest'] = [G_test_score(tkn_count, exp) for tkn_count, exp in zip(tc_dataframe[column], tc_dataframe[column+'_exp'])]\n",
        "\n",
        "    return tc_dataframe\n",
        "\n",
        "\n",
        "\n",
        "def Entropy(raw_sql):\n",
        "    p, lns = collections.Counter(str(raw_sql)), float(len(str(raw_sql)))\n",
        "    return -sum( count/lns * math.log(count/lns, 2) for count in p.values())\n",
        "\n",
        "\n",
        "def G_means(token_seq, c_name):\n",
        "    try:\n",
        "        g_scores = [tc_dataframe.loc[token][c_name] for token in token_seq]\n",
        "    except KeyError:\n",
        "        return 0\n",
        "    return sum(g_scores)/len(g_scores) if g_scores else 0 # Average\n",
        "\n",
        "\n",
        "\n",
        "# read data from file.\n",
        "############# this path change as done in colab!!###############\n",
        "basedir = 'traningdata'\n",
        "filelist = os.listdir(basedir)\n",
        "df_list = []\n",
        "for file in filelist:\n",
        "    if file == '.DS_Store':\n",
        "        continue\n",
        "    df = pd.read_csv(os.path.join(basedir,file), sep='Aw3s0meSc0t7', names=['raw_sql'], header=None, engine='python')\n",
        "    df['type'] = 'plain' if file.split('.')[0] == 'plain' else 'sqli'\n",
        "    df_list.append(df)\n",
        "\n",
        "\n",
        "dataframe = pd.concat(df_list, ignore_index=True)\n",
        "dataframe.dropna(inplace=True)\n",
        "print (dataframe['type'].value_counts())\n",
        "\n",
        "# tokenize raw sql\n",
        "dataframe['sql_tokens'] = dataframe['raw_sql'].map(lambda x: Sql_tokenizer(x))\n",
        "\n",
        "# get token sequences\n",
        "dataframe['token_seq'] = dataframe['sql_tokens'].map(lambda x: GetTokenSeq(x, 3))\n",
        "\n",
        "_tokens, _types = zip(*[(token,token_type) for token_list,token_type in zip(dataframe['token_seq'], dataframe['type']) for token in token_list])\n",
        "tc_dataframe = G_test(pd.Series(_tokens), pd.Series(_types))\n",
        "\n",
        "\n",
        "dataframe['token_length'] = dataframe['sql_tokens'].map(lambda x: len(x))\n",
        "dataframe['entropy'] = dataframe['raw_sql'].map(lambda x: Entropy(x))\n",
        "dataframe['sqli_g_means'] = dataframe['token_seq'].map(lambda x: G_means(x, 'sqli_GTest'))\n",
        "dataframe['plain_g_means'] = dataframe['token_seq'].map(lambda x: G_means(x, 'plain_GTest'))\n",
        "\n",
        "\n",
        "X = dataframe[['token_length', 'entropy','sqli_g_means','plain_g_means']].to_numpy()\n",
        "#print(X)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "labelencoder_y = LabelEncoder()\n",
        "y = labelencoder_y.fit_transform(dataframe['type'].tolist())\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state = 0)\n",
        "\n",
        "'''\n",
        "# Feature Scaling but not use this.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)\n",
        "'''\n",
        "import joblib \n",
        "from joblib import dump\n",
        "from joblib import load\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=7, random_state=0).fit(X_train, y_train)\n",
        "print (\"Gradient Boosting Tree Acurracy: %f\" % clf.score(X_test, y_test))\n",
        "joblib.dump(clf, 'models.pkl') \n",
        "\n",
        "\n",
        "def Check_is_sql(sql):\n",
        "    # do some pre-processing remoce comment /**/, /*!num */\n",
        "    _tmp = re.sub(r'(/\\*[\\w\\d(\\`|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\&|\\*|\\(|\\)|\\-|\\_|\\=|\\+|\\[|\\{|\\]|\\}|\\\\|\\:|\\;|\\'|\\\"|\\<|\\>|\\,|\\.|\\?)\\s\\r\\n\\v\\f]*\\*/)', ' ', sql)\n",
        "    _tmp = re.sub(r'(/\\*!\\d+|\\*/)', ' ', _tmp)\n",
        "\n",
        "    sql_tokens = Sql_tokenizer(_tmp.strip())\n",
        "    token_seq = GetTokenSeq(sql_tokens, 3)\n",
        "    sqli_g_means = G_means(token_seq, 'sqli_GTest')\n",
        "    plain_g_means = G_means(token_seq, 'plain_GTest')\n",
        "    _X = [[len(sql_tokens), Entropy(sql), sqli_g_means, plain_g_means]]\n",
        "    return clf.predict(_X)[0]\n",
        "\n",
        "check_data = 'select * from name where 1=1 '\n",
        "res = Check_is_sql(check_data)\n",
        "if res == 1:\n",
        "    print (\"[SQL-Injection]: %s\" % check_data)\n",
        "else:\n",
        "    print (\"[PLAIN-TEXT]: %s\" % check_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sqli     5928\n",
            "plain    3694\n",
            "Name: type, dtype: int64\n",
            "Gradient Boosting Tree Acurracy: 0.999169\n",
            "[SQL-Injection]: select * from name where 1=1 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFZLQlblL2vO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}